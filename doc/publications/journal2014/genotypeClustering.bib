% This file was created with JabRef 2.10.
% Encoding: MacRoman


@Article{1KG2012,
  Title                    = {An integrated map of genetic variation from 1,092 human genomes.},
  Author                   = {{1000 Genomes Project Consortium}},
  Journal                  = {Nature},
  Year                     = {2012},

  Month                    = {Nov},
  Number                   = {7422},
  Pages                    = {56--65},
  Volume                   = {491},

  Bdsk-url-1               = {http://dx.doi.org/10.1038/nature11632},
  Doi                      = {10.1038/nature11632},
  Keywords                 = {Alleles; Binding Sites, genetics; Conserved Sequence, genetics; Continental Population Groups, genetics; Evolution, Molecular; Genetic Variation, genetics; Genetics, Medical; Genetics, Population; Genome, Human, genetics; Genome-Wide Association Study; Genomics; Haplotypes, genetics; Humans; Nucleotide Motifs; Polymorphism, Single Nucleotide, genetics; Sequence Deletion, genetics; Transcription Factors, metabolism},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {nature11632},
  Pmid                     = {23128226},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1038/nature11632}
}

@Article{Alexander2009,
  Title                    = {Fast model-based estimation of ancestry in unrelated individuals.},
  Author                   = {Alexander, David H. and Novembre, John and Lange, Kenneth},
  Journal                  = {Genome Res},
  Year                     = {2009},

  Month                    = {Sep},
  Number                   = {9},
  Pages                    = {1655--1664},
  Volume                   = {19},

  Abstract                 = {Population stratification has long been recognized as a confounding factor in genetic association studies. Estimated ancestries, derived from multi-locus genotype data, can be used to perform a statistical correction for population stratification. One popular technique for estimation of ancestry is the model-based approach embodied by the widely applied program structure. Another approach, implemented in the program EIGENSTRAT, relies on Principal Component Analysis rather than model-based estimation and does not directly deliver admixture fractions. EIGENSTRAT has gained in popularity in part owing to its remarkable speed in comparison to structure. We present a new algorithm and a program, ADMIXTURE, for model-based estimation of ancestry in unrelated individuals. ADMIXTURE adopts the likelihood model embedded in structure. However, ADMIXTURE runs considerably faster, solving problems in minutes that take structure hours. In many of our experiments, we have found that ADMIXTURE is almost as fast as EIGENSTRAT. The runtime improvements of ADMIXTURE rely on a fast block relaxation scheme using sequential quadratic programming for block updates, coupled with a novel quasi-Newton acceleration of convergence. Our algorithm also runs faster and with greater accuracy than the implementation of an Expectation-Maximization (EM) algorithm incorporated in the program FRAPPE. Our simulations show that ADMIXTURE's maximum likelihood estimates of the underlying admixture coefficients and ancestral allele frequencies are as accurate as structure's Bayesian estimates. On real-world data sets, ADMIXTURE's estimates are directly comparable to those from structure and EIGENSTRAT. Taken together, our results show that ADMIXTURE's computational speed opens up the possibility of using a much larger set of markers in model-based ancestry estimation and that its estimates are suitable for use in correcting for population stratification in association studies.},
  Doi                      = {10.1101/gr.094052.109},
  Institution              = {Department of Biomathematics, University of California at Los Angeles, Los Angeles, California 90095, USA. dalexander@ucla.edu},
  Keywords                 = {Algorithms; Computational Biology; Europe, ethnology; Gene Frequency; Genetic Association Studies; Genetics, Population; Genotype; Humans; Inflammatory Bowel Diseases, ethnology/genetics; Jews, ethnology; Likelihood Functions; Models, Genetic; Polymorphism, Single Nucleotide; Software; Time Factors},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {gr.094052.109},
  Pmid                     = {19648217},
  Timestamp                = {2015.09.16},
  Url                      = {http://dx.doi.org/10.1101/gr.094052.109}
}

@InProceedings{ICWSM09154,
  Title                    = {Gephi: An Open Source Software for Exploring and Manipulating Networks},
  Author                   = {Mathieu Bastian and Sebastien Heymann and Mathieu Jacomy},
  Booktitle                = {Proceedings of the International AAAI Conference on Weblogs and Social Media},
  Year                     = {2009},

  Abstract                 = {Gephi is an open source software for graph and network analysis. It uses a 3D render engine to display large networks in real-time and to speed up the exploration. A flexible and multi-task architecture brings new possibilities to work with complex data sets and produce valuable visual results. We present several key features of Gephi in the context of interactive exploration and interpretation of networks. It provides easy and broad access to network data and allows for spatializing, filtering, navigating, manipulating and clustering. Finally, by presenting dynamic features of Gephi, we highlight key aspects of dynamic network visualization.},
  Bdsk-url-1               = {http://www.aaai.org/ocs/index.php/ICWSM/09/paper/view/154},
  Conference               = {International AAAI Conference on Weblogs and Social Media},
  Date-modified            = {2015-03-11 05:47:43 +0000},
  Keywords                 = {network;network science;visualization;graph exploration;open source;free software;dynamic network;interactive interface;graph;force vector;java;OpenGL;3-D visualization;user-centric;graph layout;complex graph rendering;network analysis;webatlas},
  Url                      = {http://www.aaai.org/ocs/index.php/ICWSM/09/paper/view/154}
}

@Article{ICWSM09154,
  Title                    = {Gephi: An Open Source Software for Exploring and Manipulating Networks},
  Author                   = {Mathieu Bastian and Sebastien Heymann and Mathieu Jacomy},
  Year                     = {2009},

  Abstract                 = {Gephi is an open source software for graph and network analysis. It uses a 3D render engine to display large networks in real-time and to speed up the exploration. A flexible and multi-task architecture brings new possibilities to work with complex data sets and produce valuable visual results. We present several key features of Gephi in the context of interactive exploration and interpretation of networks. It provides easy and broad access to network data and allows for spatializing, filtering, navigating, manipulating and clustering. Finally, by presenting dynamic features of Gephi, we highlight key aspects of dynamic network visualization.},
  Conference               = {International AAAI Conference on Weblogs and Social Media},
  Keywords                 = {network;network science;visualization;graph exploration;open source;free software;dynamic network;interactive interface;graph;force vector;java;OpenGL;3-D visualization;user-centric;graph layout;complex graph rendering;network analysis;webatlas},
  Url                      = {http://www.aaai.org/ocs/index.php/ICWSM/09/paper/view/154}
}

@Article{Borthakur2007,
  Title                    = {The hadoop distributed file system: Architecture and design},
  Author                   = {Borthakur, Dhruba},
  Journal                  = {Hadoop Project Website},
  Year                     = {2007},
  Number                   = {2007},
  Pages                    = {21},
  Volume                   = {11}
}

@Article{TCGA2013,
  Title                    = {The Cancer Genome Atlas Pan-Cancer analysis project.},
  Author                   = {{Cancer Genome Atlas Research Network}},
  Journal                  = {Nat Genet},
  Year                     = {2013},

  Month                    = {Oct},
  Number                   = {10},
  Pages                    = {1113--1120},
  Volume                   = {45},

  Abstract                 = {The Cancer Genome Atlas (TCGA) Research Network has profiled and analyzed large numbers of human tumors to discover molecular aberrations at the DNA, RNA, protein and epigenetic levels. The resulting rich data provide a major opportunity to develop an integrated picture of commonalities, differences and emergent themes across tumor lineages. The Pan-Cancer initiative compares the first 12 tumor types profiled by TCGA. Analysis of the molecular aberrations and their functional roles across tumor types will teach us how to extend therapies effective in one cancer type to others with a similar genomic profile.},
  Bdsk-url-1               = {http://dx.doi.org/10.1038/ng.2764},
  Doi                      = {10.1038/ng.2764},
  Keywords                 = {Gene Expression Profiling; Genome; Humans; Neoplasms, genetics/pathology},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {ng.2764},
  Pmid                     = {24071849},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1038/ng.2764}
}

@InProceedings{Chu2009,
  Title                    = {Map-Reduce for Machine Learning on Multicore},
  Author                   = {Chu, Cheng T. and Kim, Sang K. and Lin, Yi A. and Yu, Yuanyuan and Bradski, Gary R. and Ng, Andrew Y. and Olukotun, Kunle},
  Booktitle                = {NIPS},
  Year                     = {2006},
  Editor                   = {Schlkopf, Bernhard and Platt, John C. and Hoffman, Thomas},
  Pages                    = {281--288},
  Publisher                = {MIT Press},

  Added-at                 = {2009-08-17T16:24:20.000+0200},
  Bdsk-url-1               = {http://dblp.uni-trier.de/rec/bibtex/conf/nips/ChuKLYBNO06},
  Biburl                   = {http://www.bibsonomy.org/bibtex/20de668cfdfc349fe197f30c7d840ba0f/sb3000},
  Citeulike-article-id     = {2308503},
  Citeulike-linkout-0      = {http://dblp.uni-trier.de/rec/bibtex/conf/nips/ChuKLYBNO06},
  Citeulike-linkout-1      = {http://www.cs.stanford.edu/people/ang//papers/nips06-mapreducemulticore.pdf},
  Description              = {CiteULike: Map-Reduce for Machine Learning on Multicore},
  Interhash                = {96ced10b0cf1c9ce041cf2b43574656c},
  Intrahash                = {0de668cfdfc349fe197f30c7d840ba0f},
  Keywords                 = {cloud mapreduce ml},
  Posted-at                = {2008-03-07 03:16:12},
  Priority                 = {0},
  Timestamp                = {2009-08-17T16:24:20.000+0200},
  Url                      = {http://dblp.uni-trier.de/rec/bibtex/conf/nips/ChuKLYBNO06}
}

@Article{Doering2008,
  Title                    = {SeqAn an efficient, generic C++ library for sequence analysis.},
  Author                   = {D{\"o}ring, Andreas and Weese, David and Rausch, Tobias and Reinert, Knut},
  Journal                  = {BMC Bioinformatics},
  Year                     = {2008},
  Pages                    = {11},
  Volume                   = {9},

  Abstract                 = {The use of novel algorithmic techniques is pivotal to many important problems in life science. For example the sequencing of the human genome 1 would not have been possible without advanced assembly algorithms. However, owing to the high speed of technological progress and the urgent need for bioinformatics tools, there is a widening gap between state-of-the-art algorithmic techniques and the actual algorithmic components of tools that are in widespread use.To remedy this trend we propose the use of SeqAn, a library of efficient data types and algorithms for sequence analysis in computational biology. SeqAn comprises implementations of existing, practical state-of-the-art algorithmic components to provide a sound basis for algorithm testing and development. In this paper we describe the design and content of SeqAn and demonstrate its use by giving two examples. In the first example we show an application of SeqAn as an experimental platform by comparing different exact string matching algorithms. The second example is a simple version of the well-known MUMmer tool rewritten in SeqAn. Results indicate that our implementation is very efficient and versatile to use.We anticipate that SeqAn greatly simplifies the rapid development of new bioinformatics tools by providing a collection of readily usable, well-designed algorithmic components which are fundamental for the field of sequence analysis. This leverages not only the implementation of new algorithms, but also enables a sound analysis and comparison of existing algorithms.},
  Bdsk-url-1               = {http://dx.doi.org/10.1186/1471-2105-9-11},
  Doi                      = {10.1186/1471-2105-9-11},
  Institution              = {Algorithmische Bioinformatik, Institut fÂ¸r Informatik, Takustr, 9, 14195 Berlin, Germany. doering@inf.fu-berlin.de},
  Keywords                 = {Algorithms; Database Management Systems; Databases, Genetic; Programming Languages; Sequence Alignment, methods; Sequence Analysis, methods; Software; User-Computer Interface},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Owner                    = {bau04c},
  Pii                      = {1471-2105-9-11},
  Pmid                     = {18184432},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1186/1471-2105-9-11}
}

@Article{Danecek2011Variant,
  Title                    = {{The variant call format and VCFtools}},
  Author                   = {Danecek, Petr and Auton, Adam and Abecasis, Goncalo and Albers, Cornelis A. and Banks, Eric and DePristo, Mark A. and Handsaker, Robert E. and Lunter, Gerton and Marth, Gabor T. and Sherry, Stephen T. and McVean, Gilean and Durbin, Richard and 1000 Genomes Project Analysis Group},
  Journal                  = {Bioinformatics},
  Year                     = {2011},

  Month                    = aug,
  Number                   = {15},
  Pages                    = {2156--2158},
  Volume                   = {27},

  Doi                      = {10.1093/bioinformatics/btr330},
  ISSN                     = {1460-2059},
  Keywords                 = {1000genome, formats, software, variant, vcf},
  Publisher                = {Oxford University Press},
  Url                      = {http://dx.doi.org/10.1093/bioinformatics/btr330}
}

@Article{Dong2013,
  Title                    = {Leverage hadoop framework for large scale clinical informatics applications.},
  Author                   = {Dong, Xiao and Bahroos, Neil and Sadhu, Eugene and Jackson, Tommie and Chukhman, Morris and Johnson, Robert and Boyd, Andrew and Hynes, Denise},
  Journal                  = {AMIA Summits Transl Sci Proc},
  Year                     = {2013},
  Pages                    = {53},
  Volume                   = {2013},

  Institution              = {University of Illinois at Chicago, Chicago, IL.},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Owner                    = {bau04c},
  Pmid                     = {24303235},
  Timestamp                = {2014.05.01}
}

@Article{Gao2007,
  Title                    = {Human population structure detection via multilocus genotype clustering.},
  Author                   = {Gao, Xiaoyi and Starmer, Joshua},
  Journal                  = {BMC Genet},
  Year                     = {2007},
  Pages                    = {34},
  Volume                   = {8},

  Abstract                 = {We describe a hierarchical clustering algorithm for using Single Nucleotide Polymorphism (SNP) genetic data to assign individuals to populations. The method does not assume Hardy-Weinberg equilibrium and linkage equilibrium among loci in sample population individuals.We show that the algorithm can assign sample individuals highly accurately to their corresponding ethnic groups in our tests using HapMap SNP data and it is also robust to admixed populations when tested with Perlegen SNP data. Moreover, it can detect fine-scale population structure as subtle as that between Chinese and Japanese by using genome-wide high-diversity SNP loci.The algorithm provides an alternative approach to the popular STRUCTURE program, especially for fine-scale population structure detection in genome-wide association studies. This is the first successful separation of Chinese and Japanese samples using random SNP loci with high statistical support.},
  Bdsk-url-1               = {http://dx.doi.org/10.1186/1471-2156-8-34},
  Doi                      = {10.1186/1471-2156-8-34},
  Institution              = {Miami Institute for Human Genomics, University of Miami Miller School of Medicine, Miami, FL 33136, USA. xgao@med.miami.edu},
  Keywords                 = {Algorithms; Asian Continental Ancestry Group, classification/genetics; Cluster Analysis; Computer Simulation; Gene Frequency; Genetic Markers; Genetics, Population; Genotype; Humans; Models, Genetic; Polymorphism, Single Nucleotide},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Owner                    = {bau04c},
  Pii                      = {1471-2156-8-34},
  Pmid                     = {17592628},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1186/1471-2156-8-34}
}

@Article{Gourraud2014,
  Title                    = {HLA diversity in the 1000 genomes dataset.},
  Author                   = {Gourraud, Pierre-Antoine and Khankhanian, Pouya and Cereb, Nezih and Yang, Soo Young and Feolo, Michael and Maiers, Martin and Rioux, John D. and Hauser, Stephen and Oksenberg, Jorge},
  Journal                  = {PLoS One},
  Year                     = {2014},
  Number                   = {7},
  Pages                    = {e97282},
  Volume                   = {9},

  __markedentry            = {[bau04c:6]},
  Abstract                 = {The 1000 Genomes Project aims to provide a deep characterization of human genome sequence variation by sequencing at a level that should allow the genome-wide detection of most variants with frequencies as low as 1\%. However, in the major histocompatibility complex (MHC), only the top 10 most frequent haplotypes are in the 1\% frequency range whereas thousands of haplotypes are present at lower frequencies. Given the limitation of both the coverage and the read length of the sequences generated by the 1000 Genomes Project, the highly variable positions that define HLA alleles may be difficult to identify. We used classical Sanger sequencing techniques to type the HLA-A, HLA-B, HLA-C, HLA-DRB1 and HLA-DQB1 genes in the available 1000 Genomes samples and combined the results with the 103,310 variants in the MHC region genotyped by the 1000 Genomes Project. Using pairwise identity-by-descent distances between individuals and principal component analysis, we established the relationship between ancestry and genetic diversity in the MHC region. As expected, both the MHC variants and the HLA phenotype can identify the major ancestry lineage, informed mainly by the most frequent HLA haplotypes. To some extent, regions of the genome with similar genetic or similar recombination rate have similar properties. An MHC-centric analysis underlines departures between the ancestral background of the MHC and the genome-wide picture. Our analysis of linkage disequilibrium (LD) decay in these samples suggests that overestimation of pairwise LD occurs due to a limited sampling of the MHC diversity. This collection of HLA-specific MHC variants, available on the dbMHC portal, is a valuable resource for future analyses of the role of MHC in population and disease studies.},
  Doi                      = {10.1371/journal.pone.0097282},
  Institution              = {Department of Neurology, University of California San Francisco, San Francisco, California, United States of America.},
  Keywords                 = {Alleles; Databases, Genetic; Genetic Variation; Genome, Human; Genotype; HLA Antigens, genetics; Haplotypes; Histocompatibility Testing; Human Genome Project; Humans; Linkage Disequilibrium; Major Histocompatibility Complex, genetics; Polymorphism, Single Nucleotide; Principal Component Analysis},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Owner                    = {bau04c},
  Pii                      = {PONE-D-13-24434},
  Pmid                     = {24988075},
  Timestamp                = {2015.11.28},
  Url                      = {http://dx.doi.org/10.1371/journal.pone.0097282}
}

@Article{Guo2014,
  Title                    = {Cloud computing for detecting high-order genome-wide epistatic interaction via dynamic clustering.},
  Author                   = {Guo, Xuan and Meng, Yu and Yu, Ning and Pan, Yi},
  Journal                  = {BMC Bioinformatics},
  Year                     = {2014},
  Number                   = {1},
  Pages                    = {102},
  Volume                   = {15},

  Abstract                 = {Taking the advan tage of high-throughput single nucleotide polymorphism (SNP) genotyping technology, large genome-wide association studies (GWASs) have been considered to hold promise for unravelling complex relationships between genotype and phenotype. At present, traditional single-locus-based methods are insufficient to detect interactions consisting of multiple-locus, which are broadly existing in complex traits. In addition, statistic tests for high order epistatic interactions with more than 2 SNPs propose computational and analytical challenges because the computation increases exponentially as the cardinality of SNPs combinations gets larger.In this paper, we provide a simple, fast and powerful method using dynamic clustering and cloud computing to detect genome-wide multi-locus epistatic interactions. We have constructed systematic experiments to compare powers performance against some recently proposed algorithms, including TEAM, SNPRuler, EDCF and BOOST. Furthermore, we have applied our method on two real GWAS datasets, Age-related macular degeneration (AMD) and Rheumatoid arthritis (RA) datasets, where we find some novel potential disease-related genetic factors which are not shown up in detections of 2-loci epistatic interactions.Experimental results on simulated data demonstrate that our method is more powerful than some recently proposed methods on both two- and three-locus disease models. Our method has discovered many novel high-order associations that are significantly enriched in cases from two real GWAS datasets. Moreover, the running time of the cloud implementation for our method on AMD dataset and RA dataset are roughly 2 hours and 50 hours on a cluster with forty small virtual machines for detecting two-locus interactions, respectively. Therefore, we believe that our method is suitable and effective for the full-scale analysis of multiple-locus epistatic interactions in GWAS.},
  Bdsk-url-1               = {http://dx.doi.org/10.1186/1471-2105-15-102},
  Doi                      = {10.1186/1471-2105-15-102},
  Institution              = {Department of Computer Science, Georgia State University, 34 Peachtree Street, Atlanta, USA. yipan@gsu.edu.},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Owner                    = {bau04c},
  Pii                      = {1471-2105-15-102},
  Pmid                     = {24717145},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1186/1471-2105-15-102}
}

@Article{Gurdasani2015,
  Title                    = {The African Genome Variation Project shapes medical genetics in Africa.},
  Author                   = {Gurdasani, Deepti and Carstensen, Tommy and Tekola-Ayele, Fasil and Pagani, Luca and Tachmazidou, Ioanna and Hatzikotoulas, Konstantinos and Karthikeyan, Savita and Iles, Louise and Pollard, Martin O. and Choudhury, Ananyo and Ritchie, Graham R S. and Xue, Yali and Asimit, Jennifer and Nsubuga, Rebecca N. and Young, Elizabeth H. and Pomilla, Cristina and Kivinen, Katja and Rockett, Kirk and Kamali, Anatoli and Doumatey, Ayo P. and Asiki, Gershim and Seeley, Janet and Sisay-Joof, Fatoumatta and Jallow, Muminatou and Tollman, Stephen and Mekonnen, Ephrem and Ekong, Rosemary and Oljira, Tamiru and Bradman, Neil and Bojang, Kalifa and Ramsay, Michele and Adeyemo, Adebowale and Bekele, Endashaw and Motala, Ayesha and Norris, Shane A. and Pirie, Fraser and Kaleebu, Pontiano and Kwiatkowski, Dominic and Tyler-Smith, Chris and Rotimi, Charles and Zeggini, Eleftheria and Sandhu, Manjinder S.},
  Journal                  = {Nature},
  Year                     = {2015},

  Month                    = {Jan},
  Number                   = {7534},
  Pages                    = {327--332},
  Volume                   = {517},

  Abstract                 = {Given the importance of Africa to studies of human origins and disease susceptibility, detailed characterization of African genetic diversity is needed. The African Genome Variation Project provides a resource with which to design, implement and interpret genomic studies in sub-Saharan Africa and worldwide. The African Genome Variation Project represents dense genotypes from 1,481 individuals and whole-genome sequences from 320 individuals across sub-Saharan Africa. Using this resource, we find novel evidence of complex, regionally distinct hunter-gatherer and Eurasian admixture across sub-Saharan Africa. We identify new loci under selection, including loci related to malaria susceptibility and hypertension. We show that modern imputation panels (sets of reference genotypes from which unobserved or missing genotypes in study sets can be inferred) can identify association signals at highly differentiated loci across populations in sub-Saharan Africa. Using whole-genome sequencing, we demonstrate further improvements in imputation accuracy, strengthening the case for large-scale sequencing efforts of diverse African haplotypes. Finally, we present an efficient genotype array design capturing common genetic variation in Africa.},
  Doi                      = {10.1038/nature13997},
  Institution              = { Department of Public Health and Primary Care, University of Cambridge, 2 Wort's Causeway, Cambridge, CB1 8RN, UK.},
  Keywords                 = {Africa; Africa South of the Sahara; Asia, ethnology; Europe, ethnology; Genetic Variation, genetics; Genetics, Medical, trends; Genome, Human, genetics; Genomics, trends; Humans; Risk Factors; Selection, Genetic, genetics},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {nature13997},
  Pmid                     = {25470054},
  Timestamp                = {2015.09.16},
  Url                      = {http://dx.doi.org/10.1038/nature13997}
}

@Misc{Horton2013,
  Title                    = {Hortonworks Data Platform},

  Author                   = {Hortonworks},
  Note                     = {[Online; accessed 6 March 2015]},
  Year                     = {2013},

  Bdsk-url-1               = {http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.6.0/bk_installing_manually_book/bk_installing_manually_book-20131022.pdf},
  Date-added               = {2015-03-06 07:19:05 +0000},
  Date-modified            = {2015-03-11 05:40:03 +0000},
  LastChecked              = {6 March 2015},
  Url                      = {http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.0.6.0/bk_installing_manually_book/bk_installing_manually_book-20131022.pdf},
  Urldate                  = {22 October 2013}
}

@Article{Huang2013,
  Title                    = {BlueSNP: R package for highly scalable genome-wide association studies using Hadoop clusters.},
  Author                   = {Huang, Hailiang and Tata, Sandeep and Prill, Robert J.},
  Journal                  = {Bioinformatics},
  Year                     = {2013},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {135--136},
  Volume                   = {29},

  Abstract                 = {Computational workloads for genome-wide association studies (GWAS) are growing in scale and complexity outpacing the capabilities of single-threaded software designed for personal computers. The BlueSNP R package implements GWAS statistical tests in the R programming language and executes the calculations across computer clusters configured with Apache Hadoop, a de facto standard framework for distributed data processing using the MapReduce formalism. BlueSNP makes computationally intensive analyses, such as estimating empirical p-values via data permutation, and searching for expression quantitative trait loci over thousands of genes, feasible for large genotype-phenotype datasets. Availability and implementation: http://github.com/ibm-bioinformatics/bluesnp},
  Bdsk-url-1               = {http://dx.doi.org/10.1093/bioinformatics/bts647},
  Doi                      = {10.1093/bioinformatics/bts647},
  Institution              = {Healthcare Informatics, IBM Almaden Research Center, San Jose, CA 95120, USA.},
  Keywords                 = {Genome-Wide Association Study; Humans; Phenotype; Quantitative Trait Loci; Software},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {bts647},
  Pmid                     = {23202745},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1093/bioinformatics/bts647}
}

@Article{Hubert1985,
  Title                    = {Comparing partitions},
  Author                   = {Hubert, Lawrence and Arabie, Phipps},
  Journal                  = {Journal of Classification},
  Year                     = {1985},
  Number                   = {1},
  Pages                    = {193-218},
  Volume                   = {2},

  Bdsk-url-1               = {http://dx.doi.org/10.1007/BF01908075},
  Date-added               = {2014-05-17 04:09:59 +0000},
  Date-modified            = {2014-05-17 04:10:33 +0000},
  Doi                      = {10.1007/BF01908075},
  ISSN                     = {0176-4268},
  Keywords                 = {Measures of agreement; Measures of association; Consensus indices},
  Language                 = {English},
  Publisher                = {Springer-Verlag},
  Url                      = {http://dx.doi.org/10.1007/BF01908075}
}

@Article{Hunter2014,
  Title                    = {The genetics of human migrations: Our ancestors migration out of Africa has left traces in our genomes that explain how they adapted to new environments.},
  Author                   = {Hunter, Philip},
  Journal                  = {EMBO Rep},
  Year                     = {2014},

  Month                    = {Oct},
  Number                   = {10},
  Pages                    = {1019--1022},
  Volume                   = {15},

  Bdsk-url-1               = {http://dx.doi.org/10.15252/embr.201439469},
  Doi                      = {10.15252/embr.201439469},
  Institution              = {Freelance journalist in London, UK.},
  Keywords                 = {Adaptation, Biological, genetics; Africa; Biological Evolution; Continental Population Groups, genetics; Environment; Genome, Human; Human Migration; Humans},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {embr.201439469},
  Pmid                     = {25216943},
  Timestamp                = {2015.09.10},
  Url                      = {http://dx.doi.org/10.15252/embr.201439469}
}

@Article{Jourdren2012,
  Title                    = {Eoulsan: a cloud computing-based framework facilitating high throughput sequencing analyses.},
  Author                   = {Jourdren, Laurent and Bernard, Maria and Dillies, Marie-Agn{\"E}s and {Le Crom}, St{\`E}phane},
  Journal                  = {Bioinformatics},
  Year                     = {2012},

  Month                    = {Jun},
  Number                   = {11},
  Pages                    = {1542--1543},
  Volume                   = {28},

  Abstract                 = {We developed a modular and scalable framework called Eoulsan, based on the Hadoop implementation of the MapReduce algorithm dedicated to high-throughput sequencing data analysis. Eoulsan allows users to easily set up a cloud computing cluster and automate the analysis of several samples at once using various software solutions available. Our tests with Amazon Web Services demonstrated that the computation cost is linear with the number of instances booked as is the running time with the increasing amounts of data. Availability and implementation: Eoulsan is implemented in Java, supported on Linux systems and distributed under the LGPL License at: http://transcriptome.ens.fr/eoulsan/},
  Bdsk-url-1               = {http://dx.doi.org/10.1093/bioinformatics/bts165},
  Doi                      = {10.1093/bioinformatics/bts165},
  Institution              = {{\ldots}cole normale sup{\`E}rieure, Institut de Biologie de l'ENS, INSERM U1024, Paris, France. eoulsan@biologie.ens.fr},
  Keywords                 = {Algorithms; Animals; Computational Biology, methods; High-Throughput Nucleotide Sequencing, methods; Mice; Sequence Analysis, RNA, methods; Software},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {bts165},
  Pmid                     = {22492314},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1093/bioinformatics/bts165}
}

@InProceedings{Ko2014,
  Title                    = {Predicting the severity of motor neuron disease progression using electronic health record data with a cloud computing Big Data approach},
  Author                   = {Ko, Kyung Dae and Kim, Dongkyu and El-Ghazawi, Tarek and Morizono, Hiroki},
  Booktitle                = {Computational Intelligence in Bioinformatics and Computational Biology, 2014 IEEE Conference on},
  Year                     = {2014},
  Organization             = {IEEE},
  Pages                    = {1--6}
}

@Article{Laitman2013,
  Title                    = {Haplotype analysis of the 185delAG BRCA1 mutation in ethnically diverse populations.},
  Author                   = {Laitman, Yael and Feng, Bing-Jian and Zamir, Itay M. and Weitzel, Jeffrey N. and Duncan, Paul and Port, Danielle and Thirthagiri, Eswary and Teo, Soo-Hwang and Evans, Gareth and Latif, Ayse and Newman, William G. and Gershoni-Baruch, Ruth and Zidan, Jamal and Shimon-Paluch, Shani and Goldgar, David and Friedman, Eitan},
  Journal                  = {Eur J Hum Genet},
  Year                     = {2013},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {212--216},
  Volume                   = {21},

  Abstract                 = {The 185delAG* BRCA1 mutation is encountered primarily in Jewish Ashkenazi and Iraqi individuals, and sporadically in non-Jews. Previous studies estimated that this is a founder mutation in Jewish mutation carriers that arose before the dispersion of Jews in the Diaspora ~2500 years ago. The aim of this study was to assess the haplotype in ethnically diverse 185delAG* BRCA1 mutation carriers, and to estimate the age at which the mutation arose. Ethnically diverse Jewish and non-Jewish 185delAG*BRCA1 mutation carriers and their relatives were genotyped using 15 microsatellite markers and three SNPs spanning 12.5?MB, encompassing the BRCA1 gene locus. Estimation of mutation age was based on a subset of 11 markers spanning a region of ~5?MB, using a previously developed algorithm applying the maximum likelihood method. Overall, 188 participants (154 carriers and 34 noncarriers) from 115 families were included: Ashkenazi, Iraq, Kuchin-Indians, Syria, Turkey, Iran, Tunisia, Bulgaria, non-Jewish English, non-Jewish Malaysian, and Hispanics. Haplotype analysis indicated that the 185delAG mutation arose 750-1500 years ago. In Ashkenazim, it is a founder mutation that arose 61 generations ago, and with a small group of founder mutations was introduced into the Hispanic population (conversos) ~650 years ago, and into the Iraqi-Jewish community ~450 years ago. The 185delAG mutation in the non-Jewish populations in Malaysia and the UK arose at least twice independently. We conclude that the 185delAG* BRCA1 mutation resides on a common haplotype among Ashkenazi Jews, and arose about 61 generations ago and arose independently at least twice in non-Jews.},
  Bdsk-url-1               = {http://dx.doi.org/10.1038/ejhg.2012.124},
  Doi                      = {10.1038/ejhg.2012.124},
  Institution              = {The Susanne Levy Gertner Oncogenetics Unit, The Danek Gertner Institute of Human Genetics, Chaim Sheba Medical Center, Tel-Hashomer, Israel.},
  Keywords                 = {BRCA1 Protein, genetics; Ethnic Groups, genetics; Founder Effect; Genetics, Population; Haplotypes; Humans; Jews, genetics; Sequence Deletion},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {ejhg2012124},
  Pmid                     = {22763381},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1038/ejhg.2012.124}
}

@Article{Langmead2009,
  Title                    = {Searching for SNPs with cloud computing.},
  Author                   = {Langmead, Ben and Schatz, Michael C. and Lin, Jimmy and Pop, Mihai and Salzberg, Steven L.},
  Journal                  = {Genome Biol},
  Year                     = {2009},
  Number                   = {11},
  Pages                    = {R134},
  Volume                   = {10},

  Abstract                 = {As DNA sequencing outpaces improvements in computer speed, there is a critical need to accelerate tasks like alignment and SNP calling. Crossbow is a cloud-computing software tool that combines the aligner Bowtie and the SNP caller SOAPsnp. Executing in parallel using Hadoop, Crossbow analyzes data comprising 38-fold coverage of the human genome in three hours using a 320-CPU cluster rented from a cloud computing service for about $85. Crossbow is available from http://bowtie-bio.sourceforge.net/crossbow/.},
  Bdsk-url-1               = {http://dx.doi.org/10.1186/gb-2009-10-11-r134},
  Doi                      = {10.1186/gb-2009-10-11-r134},
  Institution              = {Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health, 615 North Wolfe Street, Baltimore, Maryland 21205, USA. blangmea@jhsph.edu},
  Keywords                 = {Algorithms; Alleles; Chromosomes, Human, Pair 22, genetics; Chromosomes, Human, X, genetics; Chromosomes, ultrastructure; Computational Biology, methods; Computer Simulation; Computers; Heterozygote; Humans; Models, Genetic; Polymorphism, Single Nucleotide; Sequence Analysis, DNA; Software},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {gb-2009-10-11-r134},
  Pmid                     = {19930550},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1186/gb-2009-10-11-r134}
}

@Article{Lunshof2013,
  Title                    = {Our genomes today: time to be clear.},
  Author                   = {Lunshof, Jeantine E. and Ball, Madeleine P.},
  Journal                  = {Genome Med},
  Year                     = {2013},
  Number                   = {6},
  Pages                    = {52},
  Volume                   = {5},

  Doi                      = {10.1186/gm456},
  Institution              = {Department of Genetics, Harvard Medical School, 77 Avenue Louis Pasteur, Boston, MA 02215, USA.},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Owner                    = {bau04c},
  Pii                      = {gm456},
  Pmid                     = {23806003},
  Timestamp                = {2015.08.14},
  Url                      = {http://dx.doi.org/10.1186/gm456}
}

@TechReport{Massie2013,
  Title                    = {ADAM: Genomics Formats and Processing Patterns for Cloud Scale Computing},
  Author                   = {Massie, Matt and Nothaft, Frank and Hartl, Christopher and Kozanitis, Christos and Schumacher, Andr{\`E} and Joseph, Anthony D. and Patterson, David A.},
  Institution              = {EECS Department, University of California, Berkeley},
  Year                     = {2013},
  Month                    = {Dec},
  Number                   = {UCB/EECS-2013-207},

  Bdsk-url-1               = {http://www.eecs.berkeley.edu/Pubs/TechRpts/2013/EECS-2013-207.html},
  Url                      = {http://www.eecs.berkeley.edu/Pubs/TechRpts/2013/EECS-2013-207.html}
}

@Article{McKenna2010,
  Title                    = {The Genome Analysis Toolkit: a MapReduce framework for analyzing next-generation DNA sequencing data.},
  Author                   = {McKenna, Aaron and Hanna, Matthew and Banks, Eric and Sivachenko, Andrey and Cibulskis, Kristian and Kernytsky, Andrew and Garimella, Kiran and Altshuler, David and Gabriel, Stacey and Daly, Mark and DePristo, Mark A.},
  Journal                  = {Genome Res},
  Year                     = {2010},

  Month                    = {Sep},
  Number                   = {9},
  Pages                    = {1297--1303},
  Volume                   = {20},

  Abstract                 = {Next-generation DNA sequencing (NGS) projects, such as the 1000 Genomes Project, are already revolutionizing our understanding of genetic variation among individuals. However, the massive data sets generated by NGS--the 1000 Genome pilot alone includes nearly five terabases--make writing feature-rich, efficient, and robust analysis tools difficult for even computationally sophisticated individuals. Indeed, many professionals are limited in the scope and the ease with which they can answer scientific questions by the complexity of accessing and manipulating the data produced by these machines. Here, we discuss our Genome Analysis Toolkit (GATK), a structured programming framework designed to ease the development of efficient and robust analysis tools for next-generation DNA sequencers using the functional programming philosophy of MapReduce. The GATK provides a small but rich set of data access patterns that encompass the majority of analysis tool needs. Separating specific analysis calculations from common data management infrastructure enables us to optimize the GATK framework for correctness, stability, and CPU and memory efficiency and to enable distributed and shared memory parallelization. We highlight the capabilities of the GATK by describing the implementation and application of robust, scale-tolerant tools like coverage calculators and single nucleotide polymorphism (SNP) calling. We conclude that the GATK programming framework enables developers and analysts to quickly and easily write efficient and robust NGS tools, many of which have already been incorporated into large-scale sequencing projects like the 1000 Genomes Project and The Cancer Genome Atlas.},
  Bdsk-url-1               = {http://dx.doi.org/10.1101/gr.107524.110},
  Doi                      = {10.1101/gr.107524.110},
  Institution              = {Program in Medical and Population Genetics, The Broad Institute of Harvard and MIT, Cambridge, Massachusetts 02142, USA.},
  Keywords                 = {Base Sequence; Genome; Genomics, methods; Sequence Analysis, DNA, methods; Software},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {gr.107524.110},
  Pmid                     = {20644199},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1101/gr.107524.110}
}

@Article{Nordberg2013,
  Title                    = {BioPig: a Hadoop-based analytic toolkit for large-scale sequence data.},
  Author                   = {Nordberg, Henrik and Bhatia, Karan and Wang, Kai and Wang, Zhong},
  Journal                  = {Bioinformatics},
  Year                     = {2013},

  Month                    = {Dec},
  Number                   = {23},
  Pages                    = {3014--3019},
  Volume                   = {29},

  Abstract                 = {The recent revolution in sequencing technologies has led to an exponential growth of sequence data. As a result, most of the current bioinformatics tools become obsolete as they fail to scale with data. To tackle this 'data deluge', here we introduce the BioPig sequence analysis toolkit as one of the solutions that scale to data and computation.We built BioPig on the Apache's Hadoop MapReduce system and the Pig data flow language. Compared with traditional serial and MPI-based algorithms, BioPig has three major advantages: first, BioPig's programmability greatly reduces development time for parallel bioinformatics applications; second, testing BioPig with up to 500 Gb sequences demonstrates that it scales automatically with size of data; and finally, BioPig can be ported without modification on many Hadoop infrastructures, as tested with Magellan system at National Energy Research Scientific Computing Center and the Amazon Elastic Compute Cloud. In summary, BioPig represents a novel program framework with the potential to greatly accelerate data-intensive bioinformatics analysis.},
  Bdsk-url-1               = {http://dx.doi.org/10.1093/bioinformatics/btt528},
  Doi                      = {10.1093/bioinformatics/btt528},
  Institution              = {Department of Energy, Joint Genome Institute, Walnut Creek, CA 94598, USA and Genomics Division, Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA.},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {btt528},
  Pmid                     = {24021384},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1093/bioinformatics/btt528}
}

@Article{OConnor2010,
  Title                    = {SeqWare Query Engine: storing and searching sequence data in the cloud.},
  Author                   = {O'Connor, Brian D. and Merriman, Barry and Nelson, Stanley F.},
  Journal                  = {BMC Bioinformatics},
  Year                     = {2010},
  Pages                    = {S2},
  Volume                   = {11 Suppl 12},

  Abstract                 = {Since the introduction of next-generation DNA sequencers the rapid increase in sequencer throughput, and associated drop in costs, has resulted in more than a dozen human genomes being resequenced over the last few years. These efforts are merely a prelude for a future in which genome resequencing will be commonplace for both biomedical research and clinical applications. The dramatic increase in sequencer output strains all facets of computational infrastructure, especially databases and query interfaces. The advent of cloud computing, and a variety of powerful tools designed to process petascale datasets, provide a compelling solution to these ever increasing demands.In this work, we present the SeqWare Query Engine which has been created using modern cloud computing technologies and designed to support databasing information from thousands of genomes. Our backend implementation was built using the highly scalable, NoSQL HBase database from the Hadoop project. We also created a web-based frontend that provides both a programmatic and interactive query interface and integrates with widely used genome browsers and tools. Using the query engine, users can load and query variants (SNVs, indels, translocations, etc) with a rich level of annotations including coverage and functional consequences. As a proof of concept we loaded several whole genome datasets including the U87MG cell line. We also used a glioblastoma multiforme tumor/normal pair to both profile performance and provide an example of using the Hadoop MapReduce framework within the query engine. This software is open source and freely available from the SeqWare project (http://seqware.sourceforge.net).The SeqWare Query Engine provided an easy way to make the U87MG genome accessible to programmers and non-programmers alike. This enabled a faster and more open exploration of results, quicker tuning of parameters for heuristic variant calling filters, and a common data interface to simplify development of analytical tools. The range of data types supported, the ease of querying and integrating with existing tools, and the robust scalability of the underlying cloud-based technologies make SeqWare Query Engine a nature fit for storing and searching ever-growing genome sequence datasets.},
  Bdsk-url-1               = {http://dx.doi.org/10.1186/1471-2105-11-S12-S2},
  Doi                      = {10.1186/1471-2105-11-S12-S2},
  Institution              = {UNC Lineberger Comprehensive Cancer Center, University of North Carolina, Chapel Hill, NC 27599, USA.},
  Keywords                 = {Databases, Nucleic Acid; Genome, Human; Genomics, methods; High-Throughput Nucleotide Sequencing; Humans; Sequence Analysis, DNA, methods; Software},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Owner                    = {bau04c},
  Pii                      = {1471-2105-11-S12-S2},
  Pmid                     = {21210981},
  Timestamp                = {2014.01.08},
  Url                      = {http://dx.doi.org/10.1186/1471-2105-11-S12-S2}
}

@Book{Owen2011,
  Title                    = {Mahout in Action},
  Author                   = {Owen, Sean and Anil, Robin and Dunning, Ted and Friedman, Ellen},
  Publisher                = {Manning Publications Co.},
  Year                     = {2011},

  Address                  = {Manning Publications Co. 20 Baldwin Road PO Box 261 Shelter Island, NY 11964},
  Edition                  = {First},

  Abstract                 = {This book covers machine learning using Apache Mahout. Based on experience with real-world applications, it introduces practical use cases and illustrates how Mahout can be applied to solve them. It places particular focus on issues of scalability and how to apply these techniques against large data sets using the Apache Hadoop framework. This book is written for developers familiar with Java. No prior experience with Mahout is assumed.},
  Added-at                 = {2012-03-07T16:04:47.000+0100},
  Bdsk-url-1               = {http://manning.com/owen/},
  Biburl                   = {http://www.bibsonomy.org/bibtex/2f6d421df62439bfda65253c3d8eebab7/telekoma},
  Interhash                = {a1eacf0ec14439d4042a2a7d9515f697},
  Intrahash                = {f6d421df62439bfda65253c3d8eebab7},
  Keywords                 = {action bachelor:2011:bachmann mahout},
  Timestamp                = {2012-03-07T16:04:47.000+0100},
  Url                      = {http://manning.com/owen/}
}

@Article{Paten2015,
  Title                    = {The NIH BD2K center for big data in translational genomics.},
  Author                   = {Paten, Benedict and Diekhans, Mark and Druker, Brian J. and Friend, Stephen and Guinney, Justin and Gassner, Nadine and Guttman, Mitchell and {James Kent}, W. and Mantey, Patrick and Margolin, Adam A. and Massie, Matt and Novak, Adam M. and Nothaft, Frank and Pachter, Lior and Patterson, David and Smuga-Otto, Maciej and Stuart, Joshua M. and {Van't Veer}, Laura and Wold, Barbara and Haussler, David},
  Journal                  = {J Am Med Inform Assoc},
  Year                     = {2015},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {1143--1147},
  Volume                   = {22},

  __markedentry            = {[bau04c:]},
  Abstract                 = {The world's genomics data will never be stored in a single repository - rather, it will be distributed among many sites in many countries. No one site will have enough data to explain genotype to phenotype relationships in rare diseases; therefore, sites must share data. To accomplish this, the genetics community must forge common standards and protocols to make sharing and computing data among many sites a seamless activity. Through the Global Alliance for Genomics and Health, we are pioneering the development of shared application programming interfaces (APIs) to connect the world's genome repositories. In parallel, we are developing an open source software stack (ADAM) that uses these APIs. This combination will create a cohesive genome informatics ecosystem. Using containers, we are facilitating the deployment of this software in a diverse array of environments. Through benchmarking efforts and big data driver projects, we are ensuring ADAM's performance and utility.},
  Doi                      = {10.1093/jamia/ocv047},
  Institution              = {UC Santa Cruz Genomics Institute, University of California, Santa Cruz, CA, USA Howard Hughes Medical Institute, Bethesda, MD, USA benedict@soe.ucsc.edu haussler@soe.ucsc.edu.},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {ocv047},
  Pmid                     = {26174866},
  Timestamp                = {2015.11.28},
  Url                      = {http://dx.doi.org/10.1093/jamia/ocv047}
}

@Article{Patterson2006,
  Title                    = {Population structure and eigenanalysis.},
  Author                   = {Patterson, Nick and Price, Alkes L. and Reich, David},
  Journal                  = {PLoS Genet},
  Year                     = {2006},

  Month                    = {Dec},
  Number                   = {12},
  Pages                    = {e190},
  Volume                   = {2},

  Abstract                 = {Current methods for inferring population structure from genetic data do not provide formal significance tests for population differentiation. We discuss an approach to studying population structure (principal components analysis) that was first applied to genetic data by Cavalli-Sforza and colleagues. We place the method on a solid statistical footing, using results from modern statistics to develop formal significance tests. We also uncover a general "phase change" phenomenon about the ability to detect structure in genetic data, which emerges from the statistical theory we use, and has an important implication for the ability to discover structure in genetic data: for a fixed but large dataset size, divergence between two populations (as measured, for example, by a statistic like FST) below a threshold is essentially undetectable, but a little above threshold, detection will be easy. This means that we can predict the dataset size needed to detect structure.},
  Doi                      = {10.1371/journal.pgen.0020190},
  Institution              = {Broad Institute of Harvard and MIT, Cambridge, Massachusetts, United States of America.},
  Keywords                 = {Computer Simulation, statistics /&/ numerical data; Genetic Markers; Genetic Variation; Genetics, Medical, methods/statistics /&/ numerical data; Genetics, Population, methods/statistics /&/ numerical data; Humans; Models, Genetic; Models, Statistical; Principal Component Analysis, methods},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {06-PLGE-RA-0101R3},
  Pmid                     = {17194218},
  Timestamp                = {2015.09.17},
  Url                      = {http://dx.doi.org/10.1371/journal.pgen.0020190}
}

@Article{Pugach2015,
  Title                    = {Genome-wide insights into the genetic history of human populations.},
  Author                   = {Pugach, Irina and Stoneking, Mark},
  Journal                  = {Investig Genet},
  Year                     = {2015},
  Pages                    = {6},
  Volume                   = {6},

  Abstract                 = {Although mtDNA and the non-recombining Y chromosome (NRY) studies continue to provide valuable insights into the genetic history of human populations, recent technical, methodological and computational advances and the increasing availability of large-scale, genome-wide data from contemporary human populations around the world promise to reveal new aspects, resolve finer points, and provide a more detailed look at our past demographic history. Genome-wide data are particularly useful for inferring migrations, admixture, and fine structure, as well as for estimating population divergence and admixture times and fluctuations in effective population sizes. In this review, we highlight some of the stories that have emerged from the analyses of genome-wide SNP genotyping data concerning the human history of Southern Africa, India, Oceania, Island South East Asia, Europe and the Americas and comment on possible future study directions. We also discuss advantages and drawbacks of using SNP-arrays, with a particular focus on the ascertainment bias, and ways to circumvent it.},
  Doi                      = {10.1186/s13323-015-0024-0},
  Institution              = {Department of Evolutionary Genetics, Max Planck Institute for Evolutionary Anthropology, Deutscher Platz 6, D04103 Leipzig, Germany.},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Owner                    = {bau04c},
  Pii                      = {24},
  Pmid                     = {25834724},
  Timestamp                = {2015.09.17},
  Url                      = {http://dx.doi.org/10.1186/s13323-015-0024-0}
}

@Article{Qiu2010,
  Title                    = {Hybrid cloud and cluster computing paradigms for life science applications.},
  Author                   = {Qiu, Judy and Ekanayake, Jaliya and Gunarathne, Thilina and Choi, Jong Youl and Bae, Seung-Hee and Li, Hui and Zhang, Bingjing and Wu, Tak-Lon and Ruan, Yang and Ekanayake, Saliya and Hughes, Adam and Fox, Geoffrey},
  Journal                  = {BMC Bioinformatics},
  Year                     = {2010},
  Pages                    = {S3},
  Volume                   = {11 Suppl 12},

  Abstract                 = {Clouds and MapReduce have shown themselves to be a broadly useful approach to scientific computing especially for parallel data intensive applications. However they have limited applicability to some areas such as data mining because MapReduce has poor performance on problems with an iterative structure present in the linear algebra that underlies much data analysis. Such problems can be run efficiently on clusters using MPI leading to a hybrid cloud and cluster environment. This motivates the design and implementation of an open source Iterative MapReduce system Twister.Comparisons of Amazon, Azure, and traditional Linux and Windows environments on common applications have shown encouraging performance and usability comparisons in several important non iterative cases. These are linked to MPI applications for final stages of the data analysis. Further we have released the open source Twister Iterative MapReduce and benchmarked it against basic MapReduce (Hadoop) and MPI in information retrieval and life sciences applications.The hybrid cloud (MapReduce) and cluster (MPI) approach offers an attractive production environment while Twister promises a uniform programming environment for many Life Sciences applications.We used commercial clouds Amazon and Azure and the NSF resource FutureGrid to perform detailed comparisons and evaluations of different approaches to data intensive computing. Several applications were developed in MPI, MapReduce and Twister in these different environments.},
  Bdsk-url-1               = {http://dx.doi.org/10.1186/1471-2105-11-S12-S3},
  Doi                      = {10.1186/1471-2105-11-S12-S3},
  Institution              = {School of Informatics and Computing, Indiana University, Bloomington, IN 47405, USA. xqiu@indiana.edu},
  Keywords                 = {Biological Science Disciplines; Cluster Analysis; Computational Biology, methods; Data Mining; Metagenomics; Software},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Owner                    = {bau04c},
  Pii                      = {1471-2105-11-S12-S3},
  Pmid                     = {21210982},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1186/1471-2105-11-S12-S3}
}

@InProceedings{Ranger2007,
  Title                    = {Evaluating MapReduce for Multi-core and Multiprocessor Systems},
  Author                   = {Ranger, C. and Raghuraman, R. and Penmetsa, A. and Bradski, G. and Kozyrakis, C.},
  Booktitle                = {High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on},
  Year                     = {2007},
  Month                    = {Feb},
  Pages                    = {13-24},

  Bdsk-url-1               = {http://dx.doi.org/10.1109/HPCA.2007.346181},
  Doi                      = {10.1109/HPCA.2007.346181},
  Keywords                 = {fault tolerance;multi-threading;multiprocessing systems;performance evaluation;scheduling;MapReduce;Phoenix;data partitioning;distributed system;dynamic task scheduling;error recovery;fault tolerance;functional-style code;multicore system;multiprocessor system;programming API and;shared-memory systems;thread creation;Concurrent computing;Dynamic scheduling;Fault tolerance;Laboratories;Multiprocessing systems;Parallel programming;Processor scheduling;Programming profession;Runtime;Yarn}
}

@Article{ReyesOrtiz2015121,
  Title                    = {Big Data Analytics in the Cloud: Spark on Hadoop vs MPI/OpenMP on Beowulf},
  Author                   = {Jorge L. Reyes-Ortiz and Luca Oneto and Davide Anguita},
  Journal                  = {Procedia Computer Science},
  Year                     = {2015},
  Note                     = {\{INNS\} Conference on Big Data 2015 Program San Francisco, CA, \{USA\} 8-10 August 2015},
  Pages                    = {121 - 130},
  Volume                   = {53},

  Abstract                 = {Abstract One of the biggest challenges of the current big data landscape is our inability to pro- cess vast amounts of information in a reasonable time. In this work, we explore and com- pare two distributed computing frameworks implemented on commodity cluster architectures: MPI/OpenMP on Beowulf that is high-performance oriented and exploits multi-machine/multi- core infrastructures, and Apache Spark on Hadoop which targets iterative algorithms through in-memory computing. We use the Google Cloud Platform service to create virtual machine clusters, run the frameworks, and evaluate two supervised machine learning algorithms: \{KNN\} and Pegasos SVM. Results obtained from experiments with a particle physics data set show MPI/OpenMP outperforms Spark by more than one order of magnitude in terms of processing speed and provides more consistent performance. However, Spark shows better data manage- ment infrastructure and the possibility of dealing with other aspects such as node failure and data replication. },
  Bdsk-url-1               = {http://www.sciencedirect.com/science/article/pii/S1877050915017895},
  Bdsk-url-2               = {http://dx.doi.org/10.1016/j.procs.2015.07.286},
  Doi                      = {http://dx.doi.org/10.1016/j.procs.2015.07.286},
  ISSN                     = {1877-0509},
  Keywords                 = {Parallel Computing},
  Owner                    = {bau04c},
  Timestamp                = {2015.09.21},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1877050915017895}
}

@Article{Schatz2009,
  Title                    = {CloudBurst: highly sensitive read mapping with MapReduce.},
  Author                   = {Schatz, Michael C.},
  Journal                  = {Bioinformatics},
  Year                     = {2009},

  Month                    = {Jun},
  Number                   = {11},
  Pages                    = {1363--1369},
  Volume                   = {25},

  Abstract                 = {Next-generation DNA sequencing machines are generating an enormous amount of sequence data, placing unprecedented demands on traditional single-processor read-mapping algorithms. CloudBurst is a new parallel read-mapping algorithm optimized for mapping next-generation sequence data to the human genome and other reference genomes, for use in a variety of biological analyses including SNP discovery, genotyping and personal genomics. It is modeled after the short read-mapping program RMAP, and reports either all alignments or the unambiguous best alignment for each read with any number of mismatches or differences. This level of sensitivity could be prohibitively time consuming, but CloudBurst uses the open-source Hadoop implementation of MapReduce to parallelize execution using multiple compute nodes.CloudBurst's running time scales linearly with the number of reads mapped, and with near linear speedup as the number of processors increases. In a 24-processor core configuration, CloudBurst is up to 30 times faster than RMAP executing on a single core, while computing an identical set of alignments. Using a larger remote compute cloud with 96 cores, CloudBurst improved performance by >100-fold, reducing the running time from hours to mere minutes for typical jobs involving mapping of millions of short reads to the human genome.CloudBurst is available open-source as a model for parallelizing algorithms with MapReduce at (http://cloudburst-bio.sourceforge.net/).},
  Bdsk-url-1               = {http://dx.doi.org/10.1093/bioinformatics/btp236},
  Doi                      = {10.1093/bioinformatics/btp236},
  Institution              = {Center for Bioinformatics and Computational Biology, University of Maryland, College Park, MD 20742, USA. mschatz@umiacs.umd.edu},
  Keywords                 = {Algorithms; Animals; Computational Biology, methods; DNA; Genome; Humans; Internet; Sequence Alignment; Sequence Analysis, DNA, methods},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {btp236},
  Pmid                     = {19357099},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1093/bioinformatics/btp236}
}

@Article{Schumacher2014,
  Title                    = {SeqPig: simple and scalable scripting for large sequencing data sets in Hadoop.},
  Author                   = {Schumacher, Andr{\`E} and Pireddu, Luca and Niemenmaa, Matti and Kallio, Aleksi and Korpelainen, Eija and Zanetti, Gianluigi and Heljanko, Keijo},
  Journal                  = {Bioinformatics},
  Year                     = {2014},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {119--120},
  Volume                   = {30},

  Abstract                 = {Hadoop MapReduce-based approaches have become increasingly popular due to their scalability in processing large sequencing datasets. However, as these methods typically require in-depth expertise in Hadoop and Java, they are still out of reach of many bioinformaticians. To solve this problem, we have created SeqPig, a library and a collection of tools to manipulate, analyze and query sequencing datasets in a scalable and simple manner. SeqPigscripts use the Hadoop-based distributed scripting engine Apache Pig, which automatically parallelizes and distributes data processing tasks. We demonstrate SeqPig's scalability over many computing nodes and illustrate its use with example scripts.Available under the open source MIT license at http://sourceforge.net/projects/seqpig/},
  Bdsk-url-1               = {http://dx.doi.org/10.1093/bioinformatics/btt601},
  Doi                      = {10.1093/bioinformatics/btt601},
  Institution              = {Aalto University School of Science and Helsinki Institute for Information Technology HIIT, Finland, International Computer Science Institute, Berkeley, CA, USA, CRS4-Center for Advanced Studies, Research and Development in Sardinia, Italy and CSC-IT Center for Science, Finland.},
  Keywords                 = {High-Throughput Screening Assays, methods; Software Design},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {btt601},
  Pmid                     = {24149054},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1093/bioinformatics/btt601}
}

@Article{Stein2010,
  Title                    = {The case for cloud computing in genome informatics.},
  Author                   = {Stein, Lincoln D.},
  Journal                  = {Genome Biol},
  Year                     = {2010},
  Number                   = {5},
  Pages                    = {207},
  Volume                   = {11},

  Abstract                 = {With DNA sequencing now getting cheaper more quickly than data storage or computation, the time may have come for genome informatics to migrate to the cloud.},
  Doi                      = {10.1186/gb-2010-11-5-207},
  Institution              = {Ontario Institute for Cancer Research, Toronto, ON M5G 0A3, Canada. lincoln.stein@gmail.com},
  Keywords                 = {Computational Biology, economics/methods; Genome, Human, genetics; Humans; Sequence Analysis, DNA, economics/methods},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {gb-2010-11-5-207},
  Pmid                     = {20441614},
  Timestamp                = {2015.08.14},
  Url                      = {http://dx.doi.org/10.1186/gb-2010-11-5-207}
}

@Article{Taylor2010,
  Title                    = {An overview of the Hadoop/MapReduce/HBase framework and its current applications in bioinformatics.},
  Author                   = {Taylor, Ronald C.},
  Journal                  = {BMC Bioinformatics},
  Year                     = {2010},
  Pages                    = {S1},
  Volume                   = {11 Suppl 12},

  Abstract                 = {Bioinformatics researchers are now confronted with analysis of ultra large-scale data sets, a problem that will only increase at an alarming rate in coming years. Recent developments in open source software, that is, the Hadoop project and associated software, provide a foundation for scaling to petabyte scale data warehouses on Linux clusters, providing fault-tolerant parallelized analysis on such data using a programming style named MapReduce.An overview is given of the current usage within the bioinformatics community of Hadoop, a top-level Apache Software Foundation project, and of associated open source software projects. The concepts behind Hadoop and the associated HBase project are defined, and current bioinformatics software that employ Hadoop is described. The focus is on next-generation sequencing, as the leading application area to date.Hadoop and the MapReduce programming paradigm already have a substantial base in the bioinformatics community, especially in the field of next-generation sequencing analysis, and such use is increasing. This is due to the cost-effectiveness of Hadoop-based analysis on commodity Linux clusters, and in the cloud via data upload to cloud vendors who have implemented Hadoop/HBase; and due to the effectiveness and ease-of-use of the MapReduce method in parallelization of many data analysis algorithms.},
  Bdsk-url-1               = {http://dx.doi.org/10.1186/1471-2105-11-S12-S1},
  Doi                      = {10.1186/1471-2105-11-S12-S1},
  Institution              = {Computational Biology and Bioinformatics Group, Pacific Northwest National Laboratory, Richland, Washington 99352, USA. ronald.taylor@pnl.gov},
  Keywords                 = {Algorithms; Cluster Analysis; Computational Biology, methods; High-Throughput Nucleotide Sequencing; Software},
  Language                 = {eng},
  Medline-pst              = {epublish},
  Owner                    = {bau04c},
  Pii                      = {1471-2105-11-S12-S1},
  Pmid                     = {21210976},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1186/1471-2105-11-S12-S1}
}

@Article{Wiewiorka2014,
  Title                    = {SparkSeq: fast, scalable and cloud-ready tool for the interactive genomic data analysis with nucleotide precision.},
  Author                   = {Wiewi{\'{o}}rka, Marek S. and Messina, Antonio and Pacholewska, Alicja and Maffioletti, Sergio and Gawrysiak, Piotr and Okoniewski, Micha{\l} J.},
  Journal                  = {Bioinformatics},
  Year                     = {2014},

  Month                    = {Sep},
  Number                   = {18},
  Pages                    = {2652--2653},
  Volume                   = {30},

  Abstract                 = {Many time-consuming analyses of next -: generation sequencing data can be addressed with modern cloud computing. The Apache Hadoop-based solutions have become popular in genomics BECAUSE OF: their scalability in a cloud infrastructure. So far, most of these tools have been used for batch data processing rather than interactive data querying. The SparkSeq software has been created to take advantage of a new MapReduce framework, Apache Spark, for next-generation sequencing data. SparkSeq is a general-purpose, flexible and easily extendable library for genomic cloud computing. It can be used to build genomic analysis pipelines in Scala and run them in an interactive way. SparkSeq opens up the possibility of customized ad hoc secondary analyses and iterative machine learning algorithms. This article demonstrates its scalability and overall fast performance by running the analyses of sequencing datasets. Tests of SparkSeq also prove that the use of cache and HDFS block size can be tuned for the optimal performance on multiple worker nodes.Available under open source Apache 2.0 license: https://bitbucket.org/mwiewiorka/sparkseq/.},
  Doi                      = {10.1093/bioinformatics/btu343},
  Institution              = {Institute of Computer Science, Warsaw University of Technology, Warsaw, Poland, ICS 00-665 Warsaw (MW, PG), Grid Computing Competence Center-GC3, University of Zurich, 8057 Zrich (SM, AM), Swiss Institute of Equine Medicine, Vetsuisse Faculty, University of Bern and ALP-Haras, 3001 Bern (AP), Institute of Genetics, Vetsuisse Faculty, University of Bern, Bern, 3001 Bern (AP) and Functional Genomics Center Zurich, CH-8057 Zurich, Switzerland.},
  Keywords                 = {Algorithms; Genomics, methods; High-Throughput Nucleotide Sequencing, methods; Internet; Nucleotides, genetics; Software; Statistics as Topic, methods; Time Factors},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {btu343},
  Pmid                     = {24845651},
  Timestamp                = {2015.09.17},
  Url                      = {http://dx.doi.org/10.1093/bioinformatics/btu343}
}

@InProceedings{Zaharia2011,
  Title                    = {Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing},
  Author                   = {Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and McCauley, Murphy and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  Booktitle                = {Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation},
  Year                     = {2012},
  Organization             = {USENIX Association},
  Pages                    = {2--2}
}

@Article{Zheng2014,
  Title                    = {HIBAG--HLA genotype imputation with attribute bagging.},
  Author                   = {Zheng, X. and Shen, J. and Cox, C. and Wakefield, J. C. and Ehm, M. G. and Nelson, M. R. and Weir, B. S.},
  Journal                  = {Pharmacogenomics J},
  Year                     = {2014},

  Month                    = {Apr},
  Number                   = {2},
  Pages                    = {192--200},
  Volume                   = {14},

  Abstract                 = {Genotyping of classical human leukocyte antigen (HLA) alleles is an essential tool in the analysis of diseases and adverse drug reactions with associations mapping to the major histocompatibility complex (MHC). However, deriving high-resolution HLA types subsequent to whole-genome single-nucleotide polymorphism (SNP) typing or sequencing is often cost prohibitive for large samples. An alternative approach takes advantage of the extended haplotype structure within the MHC to predict HLA alleles using dense SNP genotypes, such as those available from genome-wide SNP panels. Current methods for HLA imputation are difficult to apply or may require the user to have access to large training data sets with SNP and HLA types. We propose HIBAG, HLA Imputation using attribute BAGging, that makes predictions by averaging HLA-type posterior probabilities over an ensemble of classifiers built on bootstrap samples. We assess the performance of HIBAG using our study data (n=2668 subjects of European ancestry) as a training set and HLA data from the British 1958 birth cohort study (n=1000 subjects) as independent validation samples. Prediction accuracies for HLA-A, B, C, DRB1 and DQB1 range from 92.2\% to 98.1\% using a set of SNP markers common to the Illumina 1M Duo, OmniQuad, OmniExpress, 660K and 550K platforms. HIBAG performed well compared with the other two leading methods, HLA*IMP and BEAGLE. This method is implemented in a freely available HIBAG R package that includes pre-fit classifiers for European, Asian, Hispanic and African ancestries, providing a readily available imputation approach without the need to have access to large training data sets.},
  Bdsk-url-1               = {http://dx.doi.org/10.1038/tpj.2013.18},
  Doi                      = {10.1038/tpj.2013.18},
  Institution              = {Department of Biostatistics, University of Washington, Seattle, WA, USA.},
  Keywords                 = {Alleles; Asian Continental Ancestry Group, genetics; Drug-Related Side Effects and Adverse Reactions, genetics; European Continental Ancestry Group, genetics; Genome-Wide Association Study; Genotype; HLA Antigens, genetics; Haplotypes; Humans; Major Histocompatibility Complex, genetics; Polymorphism, Single Nucleotide},
  Language                 = {eng},
  Medline-pst              = {ppublish},
  Owner                    = {bau04c},
  Pii                      = {tpj201318},
  Pmid                     = {23712092},
  Timestamp                = {2015.09.11},
  Url                      = {http://dx.doi.org/10.1038/tpj.2013.18}
}

@Article{Zou2013,
  Title                    = {Survey of MapReduce frame operation in bioinformatics.},
  Author                   = {Zou, Quan and Li, Xu-Bin and Jiang, Wen-Rui and Lin, Zi-Yu and Li, Gui-Lin and Chen, Ke},
  Journal                  = {Brief Bioinform},
  Year                     = {2013},

  Month                    = {Feb},

  Abstract                 = {Bioinformatics is challenged by the fact that traditional analysis tools have difficulty in processing large-scale data from high-throughput sequencing. The open source Apache Hadoop project, which adopts the MapReduce framework and a distributed file system, has recently given bioinformatics researchers an opportunity to achieve scalable, efficient and reliable computing performance on Linux clusters and on cloud computing services. In this article, we present MapReduce frame-based applications that can be employed in the next-generation sequencing and other biological domains. In addition, we discuss the challenges faced by this field as well as the future works on parallel computing in bioinformatics.},
  Bdsk-url-1               = {http://dx.doi.org/10.1093/bib/bbs088},
  Doi                      = {10.1093/bib/bbs088},
  Language                 = {eng},
  Medline-pst              = {aheadofprint},
  Owner                    = {bau04c},
  Pii                      = {bbs088},
  Pmid                     = {23396756},
  Timestamp                = {2014.05.01},
  Url                      = {http://dx.doi.org/10.1093/bib/bbs088}
}

@comment{jabref-meta: selector_review:}

@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_keywords:}

